{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602f8ebf",
   "metadata": {},
   "source": [
    "### Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09127d",
   "metadata": {},
   "source": [
    "- Using the bytearray() function to convert the text to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c837c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The example_text is successfully converted to an array having numbers for each text:\n",
      "[84, 104, 105, 115, 32, 105, 115, 32, 97, 110, 32, 101, 120, 97, 109, 112, 108, 101, 32, 116, 101, 120, 116]\n"
     ]
    }
   ],
   "source": [
    "example_text = \"This is an example text\"\n",
    "barray = bytearray(example_text, encoding=\"utf-8\")\n",
    "#The example_text is converted to bytearray; Now if we return the bytearray as a list we will get the bytes number for each text\n",
    "example_text_byte_ids = list(barray)\n",
    "print(\"The example_text is successfully converted to an array having numbers for each text:\")\n",
    "print(example_text_byte_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ddc24",
   "metadata": {},
   "source": [
    "- ##### Drawback of this approach\n",
    "-       Ids is generated for every character in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1ab7ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input text: 23\n",
      "Length of IDs: 23\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of input text:\",len(example_text))\n",
    "print(\"Length of IDs:\", len(example_text_byte_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73033fae",
   "metadata": {},
   "source": [
    "- The first 256-values of BPE tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470827a",
   "metadata": {},
   "source": [
    "- The first 256-values of BPE tokenizer is single-character tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90889c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1:\"\n",
      "\n",
      "2:#\n",
      "\n",
      "3:$\n",
      "\n",
      "4:%\n",
      "\n",
      "5:&\n",
      "\n",
      "6:'\n",
      "\n",
      "7:(\n",
      "\n",
      "8:)\n",
      "\n",
      "9:*\n",
      "\n",
      "10:+\n",
      "\n",
      "11:,\n",
      "\n",
      "12:-\n",
      "\n",
      "13:.\n",
      "\n",
      "14:/\n",
      "\n",
      "15:0\n",
      "\n",
      "16:1\n",
      "\n",
      "17:2\n",
      "\n",
      "18:3\n",
      "\n",
      "19:4\n",
      "\n",
      "20:5\n",
      "\n",
      "21:6\n",
      "\n",
      "22:7\n",
      "\n",
      "23:8\n",
      "\n",
      "24:9\n",
      "\n",
      "25::\n",
      "\n",
      "26:;\n",
      "\n",
      "27:<\n",
      "\n",
      "28:=\n",
      "\n",
      "29:>\n",
      "\n",
      "30:?\n",
      "\n",
      "31:@\n",
      "\n",
      "32:A\n",
      "\n",
      "33:B\n",
      "\n",
      "34:C\n",
      "\n",
      "35:D\n",
      "\n",
      "36:E\n",
      "\n",
      "37:F\n",
      "\n",
      "38:G\n",
      "\n",
      "39:H\n",
      "\n",
      "40:I\n",
      "\n",
      "41:J\n",
      "\n",
      "42:K\n",
      "\n",
      "43:L\n",
      "\n",
      "44:M\n",
      "\n",
      "45:N\n",
      "\n",
      "46:O\n",
      "\n",
      "47:P\n",
      "\n",
      "48:Q\n",
      "\n",
      "49:R\n",
      "\n",
      "50:S\n",
      "\n",
      "51:T\n",
      "\n",
      "52:U\n",
      "\n",
      "53:V\n",
      "\n",
      "54:W\n",
      "\n",
      "55:X\n",
      "\n",
      "56:Y\n",
      "\n",
      "57:Z\n",
      "\n",
      "58:[\n",
      "\n",
      "59:\\\n",
      "\n",
      "60:]\n",
      "\n",
      "61:^\n",
      "\n",
      "62:_\n",
      "\n",
      "63:`\n",
      "\n",
      "64:a\n",
      "\n",
      "65:b\n",
      "\n",
      "66:c\n",
      "\n",
      "67:d\n",
      "\n",
      "68:e\n",
      "\n",
      "69:f\n",
      "\n",
      "70:g\n",
      "\n",
      "71:h\n",
      "\n",
      "72:i\n",
      "\n",
      "73:j\n",
      "\n",
      "74:k\n",
      "\n",
      "75:l\n",
      "\n",
      "76:m\n",
      "\n",
      "77:n\n",
      "\n",
      "78:o\n",
      "\n",
      "79:p\n",
      "\n",
      "80:q\n",
      "\n",
      "81:r\n",
      "\n",
      "82:s\n",
      "\n",
      "83:t\n",
      "\n",
      "84:u\n",
      "\n",
      "85:v\n",
      "\n",
      "86:w\n",
      "\n",
      "87:x\n",
      "\n",
      "88:y\n",
      "\n",
      "89:z\n",
      "\n",
      "90:{\n",
      "\n",
      "91:|\n",
      "\n",
      "92:}\n",
      "\n",
      "93:~\n",
      "\n",
      "94:�\n",
      "\n",
      "95:�\n",
      "\n",
      "96:�\n",
      "\n",
      "97:�\n",
      "\n",
      "98:�\n",
      "\n",
      "99:�\n",
      "\n",
      "100:�\n",
      "\n",
      "101:�\n",
      "\n",
      "102:�\n",
      "\n",
      "103:�\n",
      "\n",
      "104:�\n",
      "\n",
      "105:�\n",
      "\n",
      "106:�\n",
      "\n",
      "107:�\n",
      "\n",
      "108:�\n",
      "\n",
      "109:�\n",
      "\n",
      "110:�\n",
      "\n",
      "111:�\n",
      "\n",
      "112:�\n",
      "\n",
      "113:�\n",
      "\n",
      "114:�\n",
      "\n",
      "115:�\n",
      "\n",
      "116:�\n",
      "\n",
      "117:�\n",
      "\n",
      "118:�\n",
      "\n",
      "119:�\n",
      "\n",
      "120:�\n",
      "\n",
      "121:�\n",
      "\n",
      "122:�\n",
      "\n",
      "123:�\n",
      "\n",
      "124:�\n",
      "\n",
      "125:�\n",
      "\n",
      "126:�\n",
      "\n",
      "127:�\n",
      "\n",
      "128:�\n",
      "\n",
      "129:�\n",
      "\n",
      "130:�\n",
      "\n",
      "131:�\n",
      "\n",
      "132:�\n",
      "\n",
      "133:�\n",
      "\n",
      "134:�\n",
      "\n",
      "135:�\n",
      "\n",
      "136:�\n",
      "\n",
      "137:�\n",
      "\n",
      "138:�\n",
      "\n",
      "139:�\n",
      "\n",
      "140:�\n",
      "\n",
      "141:�\n",
      "\n",
      "142:�\n",
      "\n",
      "143:�\n",
      "\n",
      "144:�\n",
      "\n",
      "145:�\n",
      "\n",
      "146:�\n",
      "\n",
      "147:�\n",
      "\n",
      "148:�\n",
      "\n",
      "149:�\n",
      "\n",
      "150:�\n",
      "\n",
      "151:�\n",
      "\n",
      "152:�\n",
      "\n",
      "153:�\n",
      "\n",
      "154:�\n",
      "\n",
      "155:�\n",
      "\n",
      "156:�\n",
      "\n",
      "157:�\n",
      "\n",
      "158:�\n",
      "\n",
      "159:�\n",
      "\n",
      "160:�\n",
      "\n",
      "161:�\n",
      "\n",
      "162:�\n",
      "\n",
      "163:�\n",
      "\n",
      "164:�\n",
      "\n",
      "165:�\n",
      "\n",
      "166:�\n",
      "\n",
      "167:�\n",
      "\n",
      "168:�\n",
      "\n",
      "169:�\n",
      "\n",
      "170:�\n",
      "\n",
      "171:�\n",
      "\n",
      "172:�\n",
      "\n",
      "173:�\n",
      "\n",
      "174:�\n",
      "\n",
      "175:�\n",
      "\n",
      "176:�\n",
      "\n",
      "177:�\n",
      "\n",
      "178:�\n",
      "\n",
      "179:�\n",
      "\n",
      "180:�\n",
      "\n",
      "181:�\n",
      "\n",
      "182:�\n",
      "\n",
      "183:�\n",
      "\n",
      "184:�\n",
      "\n",
      "185:�\n",
      "\n",
      "186:�\n",
      "\n",
      "187:�\n",
      "\n",
      "188:\u0000\n",
      "\n",
      "189:\u0001\n",
      "\n",
      "190:\u0002\n",
      "\n",
      "191:\u0003\n",
      "\n",
      "192:\u0004\n",
      "\n",
      "193:\u0005\n",
      "\n",
      "194:\u0006\n",
      "\n",
      "195:\u0007\n",
      "\n",
      "196\n",
      "\n",
      "197:\t\n",
      "\n",
      "198:\n",
      "\n",
      "\n",
      "199:\u000b\n",
      "\n",
      "200:\f\n",
      "\n",
      "201:\n",
      "\n",
      "202:\u000e\n",
      "\n",
      "203:\u000f\n",
      "\n",
      "204:\u0010\n",
      "\n",
      "205:\u0011\n",
      "\n",
      "206:\u0012\n",
      "\n",
      "207:\u0013\n",
      "\n",
      "208:\u0014\n",
      "\n",
      "209:\u0015\n",
      "\n",
      "210:\u0016\n",
      "\n",
      "211:\u0017\n",
      "\n",
      "212:\u0018\n",
      "\n",
      "213:\u0019\n",
      "\n",
      "214:\u001a\n",
      "\n",
      "215:\u001b\n",
      "\n",
      "216:\u001c\n",
      "\n",
      "217:\u001d\n",
      "\n",
      "218:\u001e\n",
      "\n",
      "219:\u001f\n",
      "\n",
      "220: \n",
      "\n",
      "221:\n",
      "\n",
      "222:�\n",
      "\n",
      "223:�\n",
      "\n",
      "224:�\n",
      "\n",
      "225:�\n",
      "\n",
      "226:�\n",
      "\n",
      "227:�\n",
      "\n",
      "228:�\n",
      "\n",
      "229:�\n",
      "\n",
      "230:�\n",
      "\n",
      "231:�\n",
      "\n",
      "232:�\n",
      "\n",
      "233:�\n",
      "\n",
      "234:�\n",
      "\n",
      "235:�\n",
      "\n",
      "236:�\n",
      "\n",
      "237:�\n",
      "\n",
      "238:�\n",
      "\n",
      "239:�\n",
      "\n",
      "240:�\n",
      "\n",
      "241:�\n",
      "\n",
      "242:�\n",
      "\n",
      "243:�\n",
      "\n",
      "244:�\n",
      "\n",
      "245:�\n",
      "\n",
      "246:�\n",
      "\n",
      "247:�\n",
      "\n",
      "248:�\n",
      "\n",
      "249:�\n",
      "\n",
      "250:�\n",
      "\n",
      "251:�\n",
      "\n",
      "252:�\n",
      "\n",
      "253:�\n",
      "\n",
      "254:�\n",
      "\n",
      "255:�\n",
      "\n",
      "256: t\n",
      "\n",
      "257: a\n",
      "\n",
      "258:he\n",
      "\n",
      "259:in\n",
      "\n",
      "260:re\n",
      "\n",
      "261:on\n",
      "\n",
      "262: the\n",
      "\n",
      "263:er\n",
      "\n",
      "264: s\n",
      "\n",
      "265:at\n",
      "\n",
      "266: w\n",
      "\n",
      "267: o\n",
      "\n",
      "268:en\n",
      "\n",
      "269: c\n",
      "\n",
      "270:it\n",
      "\n",
      "271:is\n",
      "\n",
      "272:an\n",
      "\n",
      "273:or\n",
      "\n",
      "274:es\n",
      "\n",
      "275: b\n",
      "\n",
      "276:ed\n",
      "\n",
      "277: f\n",
      "\n",
      "278:ing\n",
      "\n",
      "279: p\n",
      "\n",
      "280:ou\n",
      "\n",
      "281: an\n",
      "\n",
      "282:al\n",
      "\n",
      "283:ar\n",
      "\n",
      "284: to\n",
      "\n",
      "285: m\n",
      "\n",
      "286: of\n",
      "\n",
      "287: in\n",
      "\n",
      "288: d\n",
      "\n",
      "289: h\n",
      "\n",
      "290: and\n",
      "\n",
      "291:ic\n",
      "\n",
      "292:as\n",
      "\n",
      "293:le\n",
      "\n",
      "294: th\n",
      "\n",
      "295:ion\n",
      "\n",
      "296:om\n",
      "\n",
      "297:ll\n",
      "\n",
      "298:ent\n",
      "\n",
      "299: n\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "#Print the first 300 tokens and their respective characters \n",
    "for i in range(1, 300):\n",
    "    decoded = tokenizer.decode([i])\n",
    "    print(f\"\\n{i}:{decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e190e1a",
   "metadata": {},
   "source": [
    "## Main BPE ALgorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e82493",
   "metadata": {},
   "source": [
    "- Split the input text into individual bytes\n",
    "- Repeatedly find & replace (merge) adjacent tokens (pairs) when they match any pair in the learned BPE merges (from highest to lowest “rank,” i.e., in the order they were learned)\n",
    "- Continue merging until no more merges can be applied\n",
    "- The final list of token IDs is the encoded output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbac85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e169d215",
   "metadata": {},
   "source": [
    "##### Example of preprocessing the text in train_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d084ffc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The space between the text is replaced with a special character:- Hello_World\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello World\"\n",
    "example_preprocessed = []\n",
    "for i, char in enumerate(text):\n",
    "    if char == \" \" and i!= 0 :\n",
    "        example_preprocessed.append(\"_\")\n",
    "    if char != \" \":\n",
    "        example_preprocessed.append(char)\n",
    "        \n",
    "example_preprocessed = \"\".join(example_preprocessed)\n",
    "print(\"The space between the text is replaced with a special character:-\", example_preprocessed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4121f50",
   "metadata": {},
   "source": [
    "## BPE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69587d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class BPEtokenizer:\n",
    "    def __init__(self):\n",
    "        \n",
    "        #Maps the token_id with token_str (e.g. {11456:\"some\"}) \n",
    "        self.vocab = {}\n",
    "        \n",
    "        #Maps token_str to token_id (e.g. {\"some\":11456})\n",
    "        self.inverse_vocab = {}\n",
    "        \n",
    "        #Dictionary containing all the merge_rules\n",
    "        self.bpe_merges = {}\n",
    "        \n",
    "        \n",
    "    def train(self, text, vocab_size, allowed_specials = {\"<|endoftext|>\"}):\n",
    "        #========We replace the space present between text with '_' inorder to preserve the word boundaries=============\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"_\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "                \n",
    "        processed_text = \"\".join(processed_text) \n",
    "        \n",
    "        \n",
    "        #==========Initialize the Vocabulary with unique characters==========\n",
    "        #Get the first 256 ASCII characters\n",
    "        unique_chars = [ chr(i) for i in range(256)]\n",
    "        \n",
    "        #Now add the unique character which are in processed_text but not in unique_chars list\n",
    "        unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "        \n",
    "        #=================Create the Vocab & Inverse_Vocab Dictionaries=================\n",
    "        self.vocab = {i : char for i,char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char : i for i, char in self.vocab.items()}\n",
    "        \n",
    "        #================Add allowed special tokens=============\n",
    "        if allowed_specials:\n",
    "            for token in allowed_specials:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "                    self.vocab[new_id] = token\n",
    "                    \n",
    "                    \n",
    "        #================Tokenize the processed text into token_IDs================\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "        \n",
    "        #================BPE Algorithm steps 1-3================\n",
    "        for new_id in tqdm(range(len(self.vocab), vocab_size), desc=\"Training BPE\"):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode= \"most\")\n",
    "            \n",
    "            if pair_id is None: #No more pair to merge; Stop the training\n",
    "                break\n",
    "            token_ids = self.replace_pairs(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "        \n",
    "        #================Update the vocabulary with the new merged_tokens================\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "        \n",
    "        \n",
    "    #=======================This function is used to load pretrained-vocabularies and Merge Rules from Open-AI Gpt-2=======================\n",
    "    def load_vocab_and_merges_from_OpenAI(self, vocab_path, bpe_merges_path):\n",
    "        \n",
    "        \n",
    "        #===================Load Vocabulary===================\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            \n",
    "            #Using the loaded_vocab's vocab and inverse_vocab dictionaries\n",
    "            self.vocab = {int(v):k for k,v in loaded_vocab.items()} #token_id:token_str\n",
    "            self.inverse_vocab = {k: int(v) for k,v in loaded_vocab.items()} #token_str : token_id\n",
    "            \n",
    "        #===================Load BPE Merge Rules===================\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            \n",
    "            #Skip the header line if present\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "                \n",
    "            for rank, line in enumerate(lines):\n",
    "                pair = tuple(line.strip().split())\n",
    "                \n",
    "                #Check if the pair contains exactly 2 entries or not\n",
    "                if(len(pair) != 2): \n",
    "                    print(f\"Line{rank +1} has more than 2 entries: {line.strip()}\")\n",
    "                    continue\n",
    "                token1, token2 = pair\n",
    "                \n",
    "                #Check if the str_tokens we get from the pair are in inverse_vocabulary or not\n",
    "                if token1 in self.inverse_vocab and token2 in self.inverse_vocab: \n",
    "                    token_id1 = self.inverse_vocab[token1]\n",
    "                    token_id2 = self.inverse_vocab[token2]\n",
    "                    merged_token = token1 + token2\n",
    "                    \n",
    "                    #Check if the string(merged_token) is in inverse_vocabulary or not\n",
    "                    if merged_token in self.inverse_vocab:\n",
    "                        merged_token_id = self.inverse_vocab[merged_token]\n",
    "                        \n",
    "                        #Updating the BPE merge rule list\n",
    "                        self.bpe_merges[(token_id1, token_id2)] = merged_token_id\n",
    "                        \n",
    "                    else:\n",
    "                        print(f\"Merged token {merged_token} not found in vocabulary; ===Skip====\")\n",
    "                else:\n",
    "                    print(f\"Skipping the pair:{pair} as both of the tokens are not in vocabulary\")\n",
    "                    \n",
    "    #========================This function is used to encode the input_text into list of token_ids at Inference========================                \n",
    "    def encode(self, text:str):\n",
    "        \"\"\" \n",
    "            This function will generate token_ids based on the BPE merge rules and the vocabularies it learned during the training   \n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        \n",
    "        #Split the text into tokens, Keeping the newline intact\n",
    "        words = text.replace(\"\\n\", \" \\n \").split()  #Make sure that the new_line seperator \"\\n\" is treated as a separate token\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            if i > 0 and not word.startswith(\"\\n\"): \n",
    "                tokens.append(\"_\" + word) #Add \"_\" to words that are after a space or \"\\n\"\n",
    "            else:\n",
    "                tokens.append(word)\n",
    "                \n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            #Check if the token is already present in the vocabulary or not\n",
    "            if token in self.inverse_vocab:\n",
    "                token_id = self.inverse_vocab[token]\n",
    "                token_ids.append(token_id)\n",
    "            else:\n",
    "                #Do subword_tokenization using BPE\n",
    "                sub_token_ids = self.tokenize_with_bpe(token)\n",
    "                token_ids.extend(sub_token_ids)\n",
    "                \n",
    "        return token_ids\n",
    "                \n",
    "        \n",
    "    #=======================This function is used to tokenize a sinlge token using BPE merge rules=======================                   \n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \n",
    "        #Tokenize the tokens into individual characters(it can be interpreted as initial token_Ids)\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocabulary: {missing_chars}\")\n",
    "        \n",
    "        can_merge = True\n",
    "        while can_merge and len(token_ids) > 1:\n",
    "            can_merge = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(token_ids) - 1:\n",
    "                pair = (token_ids[i] , token_ids[i+1])\n",
    "                if pair in self.bpe_merges:\n",
    "                    merged_token_id = self.bpe_merges[pair]\n",
    "                    new_tokens.append(merged_token_id)\n",
    "                    \n",
    "                    i += 2 #Skip the next token as it is already merged\n",
    "                    can_merge = True\n",
    "                else:\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                    i+=1\n",
    "            if i < len(token_ids):\n",
    "                new_tokens.append(token_ids[i])\n",
    "            token_ids = new_tokens\n",
    "                \n",
    "        return token_ids\n",
    "                    \n",
    "        \n",
    "    #======================This function is used to decode a list of token_ids back to text======================\n",
    "    def decode(self, token_ids):\n",
    "        decoded_string = \"\"\n",
    "        for token_id in token_ids:\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token Id {token_id} not found in Vocabulary\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token.startswith(\"_\"):\n",
    "                #Replace the \"_\" with space\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "            \n",
    "        return decoded_string  \n",
    "                \n",
    "        \n",
    "    #======================This function is used to save the vocabulary and BPE merges to a JSON file======================\n",
    "    def save_vocab_merges(self, vocab_path, merges_path):  \n",
    "        with open(vocab_path ,\"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump({k : v for k, v in self.vocab.items()}, file, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        #Save BPE Merges as a list of dictionaries\n",
    "        with open(merges_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            merges_list = [{\"pair\" : list(pair), \"new_id\" : new_id}\n",
    "                           for pair, new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list, file, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    #======================This function is used to Load the vocabulary and BPE merges from JSON files locally======================\n",
    "    def load_vocab_merges(self, vocab_path, bpe_merges_path):\n",
    "        \n",
    "        #Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            self.vocab = {int(k):v for k,v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {v:int(k) for k,v in loaded_vocab.items()}\n",
    "            \n",
    "        #Load bpe_merges\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges_list = json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair = tuple(merge[\"pair\"])\n",
    "                new_id = merge['new_id']\n",
    "                self.bpe_merges[pair] = new_id\n",
    "                \n",
    "    \n",
    "    #======================This function is used to find the most frequent pairs in the data======================\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "        \n",
    "        if not pairs:\n",
    "            return None\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key = lambda x : x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(),key=lambda x: x [1])[0]\n",
    "        else:\n",
    "            return ValueError(\"Invalid Code: Choose mode: 'most' or 'least'\")\n",
    "        \n",
    "    #======================This function is used to \n",
    "    @staticmethod\n",
    "    def replace_pairs(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982401ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15ec36ff",
   "metadata": {},
   "source": [
    "#### Example of inverse_vocab and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "795499dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:-\n",
      "{0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13', 20: '\\x14', 21: '\\x15', 22: '\\x16', 23: '\\x17', 24: '\\x18', 25: '\\x19', 26: '\\x1a', 27: '\\x1b', 28: '\\x1c', 29: '\\x1d', 30: '\\x1e', 31: '\\x1f', 32: ' ', 33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~', 127: '\\x7f', 128: '\\x80', 129: '\\x81', 130: '\\x82', 131: '\\x83', 132: '\\x84', 133: '\\x85', 134: '\\x86', 135: '\\x87', 136: '\\x88', 137: '\\x89', 138: '\\x8a', 139: '\\x8b', 140: '\\x8c', 141: '\\x8d', 142: '\\x8e', 143: '\\x8f', 144: '\\x90', 145: '\\x91', 146: '\\x92', 147: '\\x93', 148: '\\x94', 149: '\\x95', 150: '\\x96', 151: '\\x97', 152: '\\x98', 153: '\\x99', 154: '\\x9a', 155: '\\x9b', 156: '\\x9c', 157: '\\x9d', 158: '\\x9e', 159: '\\x9f', 160: '\\xa0', 161: '¡', 162: '¢', 163: '£', 164: '¤', 165: '¥', 166: '¦', 167: '§', 168: '¨', 169: '©', 170: 'ª', 171: '«', 172: '¬', 173: '\\xad', 174: '®', 175: '¯', 176: '°', 177: '±', 178: '²', 179: '³', 180: '´', 181: 'µ', 182: '¶', 183: '·', 184: '¸', 185: '¹', 186: 'º', 187: '»', 188: '¼', 189: '½', 190: '¾', 191: '¿', 192: 'À', 193: 'Á', 194: 'Â', 195: 'Ã', 196: 'Ä', 197: 'Å', 198: 'Æ', 199: 'Ç', 200: 'È', 201: 'É', 202: 'Ê', 203: 'Ë', 204: 'Ì', 205: 'Í', 206: 'Î', 207: 'Ï', 208: 'Ð', 209: 'Ñ', 210: 'Ò', 211: 'Ó', 212: 'Ô', 213: 'Õ', 214: 'Ö', 215: '×', 216: 'Ø', 217: 'Ù', 218: 'Ú', 219: 'Û', 220: 'Ü', 221: 'Ý', 222: 'Þ', 223: 'ß', 224: 'à', 225: 'á', 226: 'â', 227: 'ã', 228: 'ä', 229: 'å', 230: 'æ', 231: 'ç', 232: 'è', 233: 'é', 234: 'ê', 235: 'ë', 236: 'ì', 237: 'í', 238: 'î', 239: 'ï', 240: 'ð', 241: 'ñ', 242: 'ò', 243: 'ó', 244: 'ô', 245: 'õ', 246: 'ö', 247: '÷', 248: 'ø', 249: 'ù', 250: 'ú', 251: 'û', 252: 'ü', 253: 'ý', 254: 'þ', 255: 'ÿ'}\n",
      "Inverse Vocabulary:- \n",
      "{'\\x00': 0, '\\x01': 1, '\\x02': 2, '\\x03': 3, '\\x04': 4, '\\x05': 5, '\\x06': 6, '\\x07': 7, '\\x08': 8, '\\t': 9, '\\n': 10, '\\x0b': 11, '\\x0c': 12, '\\r': 13, '\\x0e': 14, '\\x0f': 15, '\\x10': 16, '\\x11': 17, '\\x12': 18, '\\x13': 19, '\\x14': 20, '\\x15': 21, '\\x16': 22, '\\x17': 23, '\\x18': 24, '\\x19': 25, '\\x1a': 26, '\\x1b': 27, '\\x1c': 28, '\\x1d': 29, '\\x1e': 30, '\\x1f': 31, ' ': 32, '!': 33, '\"': 34, '#': 35, '$': 36, '%': 37, '&': 38, \"'\": 39, '(': 40, ')': 41, '*': 42, '+': 43, ',': 44, '-': 45, '.': 46, '/': 47, '0': 48, '1': 49, '2': 50, '3': 51, '4': 52, '5': 53, '6': 54, '7': 55, '8': 56, '9': 57, ':': 58, ';': 59, '<': 60, '=': 61, '>': 62, '?': 63, '@': 64, 'A': 65, 'B': 66, 'C': 67, 'D': 68, 'E': 69, 'F': 70, 'G': 71, 'H': 72, 'I': 73, 'J': 74, 'K': 75, 'L': 76, 'M': 77, 'N': 78, 'O': 79, 'P': 80, 'Q': 81, 'R': 82, 'S': 83, 'T': 84, 'U': 85, 'V': 86, 'W': 87, 'X': 88, 'Y': 89, 'Z': 90, '[': 91, '\\\\': 92, ']': 93, '^': 94, '_': 95, '`': 96, 'a': 97, 'b': 98, 'c': 99, 'd': 100, 'e': 101, 'f': 102, 'g': 103, 'h': 104, 'i': 105, 'j': 106, 'k': 107, 'l': 108, 'm': 109, 'n': 110, 'o': 111, 'p': 112, 'q': 113, 'r': 114, 's': 115, 't': 116, 'u': 117, 'v': 118, 'w': 119, 'x': 120, 'y': 121, 'z': 122, '{': 123, '|': 124, '}': 125, '~': 126, '\\x7f': 127, '\\x80': 128, '\\x81': 129, '\\x82': 130, '\\x83': 131, '\\x84': 132, '\\x85': 133, '\\x86': 134, '\\x87': 135, '\\x88': 136, '\\x89': 137, '\\x8a': 138, '\\x8b': 139, '\\x8c': 140, '\\x8d': 141, '\\x8e': 142, '\\x8f': 143, '\\x90': 144, '\\x91': 145, '\\x92': 146, '\\x93': 147, '\\x94': 148, '\\x95': 149, '\\x96': 150, '\\x97': 151, '\\x98': 152, '\\x99': 153, '\\x9a': 154, '\\x9b': 155, '\\x9c': 156, '\\x9d': 157, '\\x9e': 158, '\\x9f': 159, '\\xa0': 160, '¡': 161, '¢': 162, '£': 163, '¤': 164, '¥': 165, '¦': 166, '§': 167, '¨': 168, '©': 169, 'ª': 170, '«': 171, '¬': 172, '\\xad': 173, '®': 174, '¯': 175, '°': 176, '±': 177, '²': 178, '³': 179, '´': 180, 'µ': 181, '¶': 182, '·': 183, '¸': 184, '¹': 185, 'º': 186, '»': 187, '¼': 188, '½': 189, '¾': 190, '¿': 191, 'À': 192, 'Á': 193, 'Â': 194, 'Ã': 195, 'Ä': 196, 'Å': 197, 'Æ': 198, 'Ç': 199, 'È': 200, 'É': 201, 'Ê': 202, 'Ë': 203, 'Ì': 204, 'Í': 205, 'Î': 206, 'Ï': 207, 'Ð': 208, 'Ñ': 209, 'Ò': 210, 'Ó': 211, 'Ô': 212, 'Õ': 213, 'Ö': 214, '×': 215, 'Ø': 216, 'Ù': 217, 'Ú': 218, 'Û': 219, 'Ü': 220, 'Ý': 221, 'Þ': 222, 'ß': 223, 'à': 224, 'á': 225, 'â': 226, 'ã': 227, 'ä': 228, 'å': 229, 'æ': 230, 'ç': 231, 'è': 232, 'é': 233, 'ê': 234, 'ë': 235, 'ì': 236, 'í': 237, 'î': 238, 'ï': 239, 'ð': 240, 'ñ': 241, 'ò': 242, 'ó': 243, 'ô': 244, 'õ': 245, 'ö': 246, '÷': 247, 'ø': 248, 'ù': 249, 'ú': 250, 'û': 251, 'ü': 252, 'ý': 253, 'þ': 254, 'ÿ': 255}\n",
      "Example of token_ids is:-[72, 101, 108, 108, 111, 95, 87, 111, 114, 108, 100]\n"
     ]
    }
   ],
   "source": [
    "example_unique_chars = [chr(i) for i in range(256)]\n",
    "vocab_example = {i:char for i, char in enumerate(example_unique_chars)}\n",
    "inverse_vocab_example = {char:i for i,char in vocab_example.items()}\n",
    "\n",
    "print(f\"Vocabulary:-\\n{vocab_example}\")\n",
    "print(f\"Inverse Vocabulary:- \\n{inverse_vocab_example}\")\n",
    "\n",
    "example_token_ids = [inverse_vocab_example[char] for char in example_preprocessed]\n",
    "print(f\"Example of token_ids is:-{example_token_ids}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006fcd83",
   "metadata": {},
   "source": [
    "#### Example of logic behind appending \"_\" in encode function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c3f933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey,', '_I', '_am', '_a', '_cat', '_,', '_I', '_like', '_to', '_sleep']\n"
     ]
    }
   ],
   "source": [
    "example_tokens = []\n",
    "example_text = \"Hey, I am a cat \\n, I like to sleep \\n\"\n",
    "example_words = example_text.replace(\"\\n\", \" \\n \").split()\n",
    "\n",
    "for i,word in enumerate(example_words):\n",
    "    if i > 0 and not word.startswith(\"\\n\"):\n",
    "        example_tokens.append(\"_\" + word)\n",
    "    else:\n",
    "        example_tokens.append(word)\n",
    "\n",
    "print(example_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679853b",
   "metadata": {},
   "source": [
    "## Algorithm Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6749b6",
   "metadata": {},
   "source": [
    "#### Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a37dc828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Data successfully loaded =======================\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
      "\n",
      "Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome \"obituary\" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of \"Gisburns\" went up.\n",
      "\n",
      "It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"./data/the-verdict.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "    \n",
    "print(\"=\"*23, f\"Data successfully loaded\", \"=\"*23)\n",
    "print(data[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6cb7234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text: 20479\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the text: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5a51b",
   "metadata": {},
   "source": [
    "### Initializing the tokenizer and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f64e07c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training BPE: 100%|██████████| 743/743 [00:04<00:00, 174.74it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BPEtokenizer()\n",
    "tokenizer.train(data, vocab_size=1000, allowed_specials={\"<|endoftext|>\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6385e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary of tokenizer:- 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of Vocabulary of tokenizer:- {len(tokenizer.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "592aa6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of merge operations performed:- 743\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of merge operations performed:- {len(tokenizer.bpe_merges)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26568a3",
   "metadata": {},
   "source": [
    "- Encoding a random text using the encode method of our tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57a1e2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 95, 653, 529, 301, 310, 95, 295, 97, 466, 121, 591, 837, 116, 286, 467, 95, 325, 973]\n"
     ]
    }
   ],
   "source": [
    "random_text = \"Jack embraced beauty through art and life\"\n",
    "token_ids = tokenizer.encode(random_text)\n",
    "print(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d468dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in random_text: 41\n",
      "Number of token Ids:- 19\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of characters in random_text: {len(random_text)}\")\n",
    "print(f\"Number of token Ids:- {len(token_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87bb7ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack embraced beauty through art and life\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8ee933b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424-->Jack\n",
      "95--> \n",
      "653-->em\n",
      "529-->br\n",
      "301-->ac\n",
      "310-->ed\n",
      "95--> \n",
      "295-->be\n",
      "97-->a\n",
      "466-->ut\n",
      "121-->y\n",
      "591--> through\n",
      "837--> ar\n",
      "116-->t\n",
      "286--> a\n",
      "467-->nd\n",
      "95--> \n",
      "325-->li\n",
      "973-->fe\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    print(f\"{token_id}-->{tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceae87f",
   "metadata": {},
   "source": [
    "- A 41 character sequence is encoded into 20 token Ids, effectively cutting the input length roughly in half compared to character-byte-based encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e68675",
   "metadata": {},
   "source": [
    "## Saving & Loading the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9109eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the trained tokenizer\n",
    "tokenizer.save_vocab_merges(vocab_path=\"./save_files/vocab.json\", merges_path=\"./save_files/merges.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e9a4339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Tokenizer loaded successfully =======================\n"
     ]
    }
   ],
   "source": [
    "#Loading the trained tokenizer\n",
    "tokenizer2 = BPEtokenizer()\n",
    "\n",
    "tokenizer2.load_vocab_merges(vocab_path=\"./save_files/vocab.json\",bpe_merges_path=\"./save_files/merges.json\")\n",
    "print(\"=\"*23, \"Tokenizer loaded successfully\", \"=\"*23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b48a1c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 286, 467, 95, 74, 554, 108, 95, 471, 110, 116, 942, 286, 95, 276, 352, 286, 467, 464, 391, 95, 681, 119, 110, 46]\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Jack and Jill went up a hill and fell down.\"\n",
    "token_ids2 = tokenizer2.encode(example_text)\n",
    "print(token_ids2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
